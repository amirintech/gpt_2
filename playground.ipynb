{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Packages"
      ],
      "metadata": {
        "id": "mydHYetn4ghg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "id": "lH23lPu_KA-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "raqih_FerwcI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4mXozGQvR0WB"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf_P-kbo5TcQ",
        "outputId": "4fe98f72-9d01-48d6-d7b0-ca56f2e31be9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d3dc8155dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBpvXKEUAGTi",
        "outputId": "74bf0c16-a2c5-4404-b085-5161cca5e33c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-05 17:49:13--  https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-04-05 17:49:13 (115 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "corpus[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H4rj9s-zANij",
        "outputId": "9ece46f4-731d-4a26-e7e5-23f8b6ea665d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Model"
      ],
      "metadata": {
        "id": "nDygWDvh4kG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2_SMALL_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,       # Max sequence length (n_positions)\n",
        "    \"embedding_dim\": 768,         # Hidden size (n_embd)\n",
        "    \"n_attention_heads\": 12,      # Number of attention heads\n",
        "    \"n_layers\": 12,               # Number of transformer blocks\n",
        "    \"dropout_rate\": 0.1,\n",
        "    \"qkv_bias\": True\n",
        "}"
      ],
      "metadata": {
        "id": "gMrUqjB4SHAq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, embedding_dim, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(embedding_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(embedding_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
        "        var = torch.var(x, dim=-1, keepdim=True)\n",
        "        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * normalized_x + self.shift"
      ],
      "metadata": {
        "id": "TX5VT2JVR8xZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(GELU, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) *\n",
        "                                         (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "nR2gJKpKT0ZD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "fiuQZKlIX3Yu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, context_length, n_heads, dropout, qkv_bias):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert(out_dim % n_heads == 0), \"out_dim must be divisible by n_heads\"\n",
        "\n",
        "        self.out_dim = out_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = out_dim // n_heads\n",
        "        self.query_weights = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
        "        self.key_weights = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
        "        self.value_weights = nn.Linear(in_dim, out_dim, bias=qkv_bias)\n",
        "        self.out_weights = nn.Linear(out_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, context_length, in_dim = x.shape\n",
        "\n",
        "        queries = self.query_weights(x)\n",
        "        keys = self.key_weights(x)\n",
        "        values = self.value_weights(x)\n",
        "\n",
        "        queries = queries.view(batch_size, context_length, self.n_heads, self.head_dim)\n",
        "        keys = keys.view(batch_size, context_length, self.n_heads, self.head_dim)\n",
        "        values = values.view(batch_size, context_length, self.n_heads, self.head_dim)\n",
        "\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        scores = queries @ keys.transpose(-2, -1)\n",
        "        mask_bool = self.mask.bool()[:context_length, :context_length]\n",
        "        scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attention_weights = torch.softmax(scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        context_vec = attention_weights @ values\n",
        "        context_vec = context_vec.transpose(1, 2).contiguous().view(\n",
        "            batch_size, context_length, self.out_dim\n",
        "        )\n",
        "        context_vec = self.out_weights(context_vec)\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "azATW_Q9YPUK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(\n",
        "            cfg[\"embedding_dim\"],\n",
        "            cfg[\"embedding_dim\"],\n",
        "            cfg[\"context_length\"],\n",
        "            cfg[\"n_attention_heads\"],\n",
        "            cfg[\"dropout_rate\"],\n",
        "            cfg[\"qkv_bias\"],\n",
        "        )\n",
        "        self.feed_forward = FeedForward(cfg[\"embedding_dim\"])\n",
        "        self.layer_norm1 = LayerNorm(cfg[\"embedding_dim\"])\n",
        "        self.layer_norm2 = LayerNorm(cfg[\"embedding_dim\"])\n",
        "        self.dropout = nn.Dropout(cfg[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.dropout(x)\n",
        "        x = shortcut + x\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.feed_forward(x)\n",
        "        x = self.dropout(x)\n",
        "        x = shortcut + x\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "YhmDdcaUlIVU"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(GPT, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_dim\"])\n",
        "        self.position_embedding = nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_dim\"])\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "        self.dropout = nn.Dropout(cfg[\"dropout_rate\"])\n",
        "        self.layer_norm = LayerNorm(cfg[\"embedding_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"embedding_dim\"], cfg[\"vocab_size\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, context_length = x.shape\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "        position_embeddings = self.position_embedding(\n",
        "            torch.arange(context_length, dtype=torch.long, device=x.device))\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.layer_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "jf0ZXs36mQsi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def decode_tokens(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "ZHaPQPqxqdmU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Corpus"
      ],
      "metadata": {
        "id": "KyaKQi6E4DYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "F20vi-t44VV7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(corpus, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDataset(corpus, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "wTVYL1Vm3jot"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.9 * len(corpus))\n",
        "train_text, val_text = corpus[:train_size], corpus[train_size:]"
      ],
      "metadata": {
        "id": "iKpThONsAv1R"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader(\n",
        "    train_text,\n",
        "    batch_size=2,\n",
        "    max_length=GPT2_SMALL_CONFIG[\"context_length\"],\n",
        "    stride=GPT2_SMALL_CONFIG[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "val_loader = create_dataloader(\n",
        "    val_text,\n",
        "    batch_size=2,\n",
        "    max_length=GPT2_SMALL_CONFIG[\"context_length\"],\n",
        "    stride=GPT2_SMALL_CONFIG[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "Eyckz3mI5HFO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(\n",
        "        logits.flatten(0, 1), target_batch.flatten())\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "E1i7a8Fr5r4T"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "FxnsFnlI6laE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_loss = calc_loss(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "KV-CIFz0PJfq"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size,\n",
        "             temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(\n",
        "                logits < min_val,\n",
        "                torch.tensor(float('-inf')).to(logits.device),\n",
        "                logits\n",
        "            )\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "iEPufkoadRlr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader,\n",
        "                       optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context):\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, \"\n",
        "                      f\"Val loss {val_loss:.3f}\"\n",
        "                )\n",
        "\n",
        "                generate(\n",
        "                    model, start_context, 50, GPT2_SMALL_CONFIG[\"context_length\"],\n",
        "                    temperature=1.3\n",
        "                )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "bUKOCfi_7CCA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = GPT(GPT2_SMALL_CONFIG).to(device)\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)"
      ],
      "metadata": {
        "id": "fdxdM2iOPavG"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    5, 100, 100, encode_text('Alone, wind howled', tokenizer).to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCbDUtoUIb2D",
        "outputId": "f506e588-591f-4d25-bf41-ff2c5e200291"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.287, Val loss 9.207\n",
            "Ep 1 (Step 000100): Train loss 5.835, Val loss 6.019\n",
            "Ep 2 (Step 000200): Train loss 5.255, Val loss 5.604\n",
            "Ep 3 (Step 000300): Train loss 4.846, Val loss 5.495\n",
            "Ep 3 (Step 000400): Train loss 4.562, Val loss 5.272\n",
            "Ep 4 (Step 000500): Train loss 4.310, Val loss 5.153\n",
            "Ep 5 (Step 000600): Train loss 4.053, Val loss 5.171\n",
            "Ep 5 (Step 000700): Train loss 3.813, Val loss 5.141\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([9.286900596618652,\n",
              "  5.835445618629455,\n",
              "  5.255203905105591,\n",
              "  4.846249089241028,\n",
              "  4.561634974479675,\n",
              "  4.309967260360718,\n",
              "  4.053466455936432,\n",
              "  3.8125492548942566],\n",
              " [9.206994745466444,\n",
              "  6.019253810246785,\n",
              "  5.604188892576429,\n",
              "  5.495038323932224,\n",
              "  5.271844651963976,\n",
              "  5.15303021007114,\n",
              "  5.17082945505778,\n",
              "  5.140641530354817],\n",
              " [2048, 206848, 411648, 616448, 821248, 1026048, 1230848, 1435648])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'gpt2-small.pth')"
      ],
      "metadata": {
        "id": "1C-jmwtmHc1B"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate(\n",
        "    model, encode_text('Alone, wind howled', tokenizer).to(device), 50, GPT2_SMALL_CONFIG[\"context_length\"],\n",
        "    temperature=1.3)\n",
        "\n",
        "print(decode_tokens(res, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sot2wbouHi5G",
        "outputId": "f7817c8c-7420-4d9b-c050-1dcf951e2490"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alone, wind howled Hermione was; circling do meet welcome.\n",
            "PETERIA:\n",
            "se limit meet without fellowship, whom he-- Reaper v felonapWound for what stay and patience.\n",
            "pherd:\n",
            "Every thousand otherft a king nor sentenced York shall lie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = generate(\n",
        "    model,\n",
        "    encode_text('Once upon a time', tokenizer).to(device),\n",
        "    50, GPT2_SMALL_CONFIG[\"context_length\"],\n",
        "    temperature=1.3)\n",
        "\n",
        "print(decode_tokens(res, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNk5ihV0Hreu",
        "outputId": "ce937c97-1792-40a7-ebf5-4dd007f5eb75"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time\n",
            "Plagles SRver I am the prince from dark?\n",
            "Ay, instruct sovereign worsories tunes conj torts such deceit and tumultuous prodig your bed in craftpy.\n",
            "\n",
            "Thy time the chalmost M blackmail infection think can Warwick shall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    1, 100, 100, encode_text('Alone, wind howled', tokenizer).to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFG380xEH2mB",
        "outputId": "2eff4d92-452a-48b8-d130-5ef4d49bf2b7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 3.780, Val loss 5.194\n",
            "Ep 1 (Step 000100): Train loss 3.468, Val loss 5.194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3.7798954343795774, 3.46810697555542],\n",
              " [5.193729877471924, 5.194102552202013],\n",
              " [2048, 206848])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNQ8SLqSIH0A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}